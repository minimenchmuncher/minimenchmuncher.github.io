---
layout: page
date: "2017-07-03T22:30:00Z"
layout: post
path: "/r-testing/"
tags:
  - R
  - Data Science
  - testing
---

### Testing with R

If you're not familiar with the `testthat` package for R, you really should be. Go
ahead, download it now. Then complete reading.

In an earlier post I
talked about why using packages is a good idea for even somewhat larger projects and
that. A basic refresher is that you should run one test that give you an expected
result as well as one test that throws an error, for each function that you write.
And if the function contains multiple methods, they should each be tested.

### Organizing tests
This is a pain, and there are no hard and fast rules. I'm really sorry to say that.
Each project is going to be different, and you can start wherever you want. Hadley,
in [his book on R Packages](http://r-pkgs.had.co.nz/tests.html) says that "you should find a happy medium that works for you." I respecfully disagree (sorry Hadley). I say, start from the standpoints that

a. you really can't run tests individually
b. you also can't debug tests (as opposed to other languages like C#)
c. you can filter tests, but only by filename.

Given these 3 restrictions, I usually err on the side of more files than fewer. The reason is, it really doesn't matter; but it gives you more options when you're trying to filter. That just requires you to be strict with your file names. For example, you might have files like:

```
test_dal_xl_read_specific.R
```

Breaking this down, the file name has to start with "test". I'll follow that with the type of code I'm testing, (here, "dal"), then more specifics about the test (for examlple, since this is a DAL test, what type of data operation, and what kind of data source), followed up by a specific indicator about the test. That way, if I want to run only this one test, I can run

```R
devtools::test(filter = "specific")
```

But if I'm trying to run all of dal tests, I could do

```R
devtools::test(filter = "dal")
```

Or anything in between.

### Setting up Your Testing Environment

Now here's the kicker with my own organizational scheme. Ther are plenty of applications for testing that are very vanilla. You have your code run on some known inputs, then have it expect results. Things get trickier when you don't want to hard-code your known inputs because a. they're too complicated, b. they're good enough to be standardized across all your tests, or c. you are trying to do something else clever.

By clever, I mean oftentimes with packages, I'll have a list of general inputs or settings in a yaml file, and a function or two that reads these inputs. If this is the type of structure you are using, you can write multiple sections, including one for your tests. The only issue with that is your settings object needs to be an input for pretty much all of your functions.

This is a scheme that I employ frequently with moderately-sized projects that have a set number of inputs. I generally like this scheme; however, it doesn't always work well. For example, if you don't know how many input files you have, or what they're named, that makes having a settings file very cumbersome and not useful.

Furthermore, having settings really just helps set up your testing environment apart from setting up your production environment. Sometimes, you want to destroy it as well, and that can make things challenging.

### Using package settings
Okay, so I've talked about using package settings. Here's an example of what I mean.

```yaml
# settings.yaml
setting_a: "universal setting"

Environment:
  Production:
    setting_b: "production setting"
	
  Testing:
    setting_b: "testing setting"
  
```

Obviously you can organize this file however you like. I like putting universal settings at the beginning, then splitting it out by production environment settings and testing environment settings (note, I mean usage environment, not an R env). I put this file in the `inst` directory in the package so it gets compiled in.

Now, you'll need some function to read these settings:

```R
readPackageSettings <- function(x = system.file('settings.yaml', package = "thispackage"), environment = "Production") {
  all_settings <- yaml::yaml.load_file(x)
  settings <- all_settings[which(names(all_settings) != "Environment")]
  append(settings, all_settings$Environment[[environment
```

This `read` function only keeps the settings that you're using in the `environment`, and flattens it so that it appends the universal settings directly with ones specific to your run environment. For example, in the `Production` environment here, you would get

```R
$setting_a
[1] "universal setting"

$setting_b
[2] "production setting"
```

Additionally, this gives you the flexibility to have as many environments you want.

The only issue with using a settings object in this way is that the vast majority of your functions will need an additional input,

```R
my_function <- function(x, settings) {...}
```


### Using helpers.R

Sometimes having a settings file is a really great idea. Your `readPackageSettings` function can be really fancy too, with checks to make sure your runtime would actually work (ie, ensure you have an internet connection if you need one), that the necessary settings have values, those kinds of things.

These settings can certainly help you run your tests in a safe testing environment. However, they're most useful when there is multiplicity in them - that is, if for each of your inputs, you have a production *and* a testing setting to be using. However, in other circumstances, you might have one testing setting, and an unknown number of pruction settings. Like for example, if you're trying to load data from forms that you're receiving, you might mock one up as a test.

In those kinds of situations, you can include a `helpers.R` file in your `testthat` directory. This file essentially gets sourced before your tests get run, letting you run code before any tests get run. That lets you set up some settings to test your code.

Where does this code go? Why, it goes in the `package:<packagename>` namespace, of course. The reason I mention that is that you can add `setUp()` and `tearDown()` types of functions in `helpers.R`.

For example, if you're in a situation where you're testing data transferrence to a database, your `setUp()` function could help set up the testing schema, the `tearDown()` function could remove the testing schema.

### Wrapping

Of course, there are always other options for setting up and taking down tests. For exmaple, you may choose to either wrap `test_that`:

```R
my_test_that <- function(desc, code) {
  setUpCode()
  test_that(desc, code)
  tearDownCode()
}
```

That will run your set up code before executing each test, and tear down after executing each test.

You can also wrap `devtools::test`:

```R
my_test <- function(pkg = ".", filter = NULL, ...) {
  setUpCode()
  devtools::test(pkg = ".", filter = NULL, ...)
  tearDownCode()
}
```

I think that this was Hadley's intention of how to do set ups and tear downs using `testthat`. I respectfully disagree though. I generally prefer code that uses the code as-is. I'll wrap my own functions. I'll clean up my code and make it look gorgeous. However, I *don't* want to customize the coding and development *tools*. Because then, realistically speaking, you should have to write tests about them.
